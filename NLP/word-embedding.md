# Word Embedding

- Used to represent words for text analysis
- Typically in the form of real-valued vector
- The vector encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.

<br><br>

```
            Word Embeddings
                    |
                   / \
                  /   \
                 /     \
                /       \
        Count (or)    Deep Learning Models
        Frequency               |
            |                   |
            |                   |
1. One Hot Encoding          Word2Vec
2. Bag of Words                / \
3. TF - IDF                   /   \
                             /     \
                          CBoW    Skipgram
    (Continuous Bag of Words)
```     
